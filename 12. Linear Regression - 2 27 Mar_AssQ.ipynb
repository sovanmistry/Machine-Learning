{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d6c3a2-593f-4d8b-bf39-a4bca552c5f9",
   "metadata": {},
   "source": [
    "### 27 Mar_AssQ LINEAR REGRESSION - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b4462b-0fae-439c-b4c3-24820e983e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "848aafc1-1d52-40d8-a5f3-7750fdd0f598",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?\n",
    "\n",
    "ans: \n",
    "    \n",
    "R-Squered:\n",
    "    \n",
    "    R-squared, also known as the coefficient of determination, is a statistical measure that represents the proportion of variation in the dependent variable that is explained by the independent variable(s) in a linear regression model. In other words, it tells us how well the regression model fits the data.\n",
    "\n",
    "Calculation: \n",
    "    \n",
    "    R-squared is calculated as the ratio of the explained variance(SSres) to the total variance(SStot):\n",
    "\n",
    "    R-squared = 1 - (SSres / SStot)\n",
    "    \n",
    "Representaion : \n",
    "    \n",
    "    The performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc214fd-0840-40dc-b85e-18935aedcca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eafab8c8-c01b-4f0a-b228-ab4c8fedd9ba",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "Ans: \n",
    "    \n",
    "Adjusted R-squared:   \n",
    "    \n",
    "    Adjusted R-squared is a modified version of the R-squared statistic that takes into account the number of independent variables in a linear regression model. While R-squared measures the proportion of variance in the dependent variable that is explained by the independent variables, adjusted R-squared provides a more accurate measure of the goodness of fit of a regression model by penalizing the inclusion of additional independent variables that do not improve the model's performance.\n",
    "\n",
    "    The formula for adjusted R-squared is:\n",
    "\n",
    "    Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - p - 1)]\n",
    "    \n",
    "    n = number of datapoint\n",
    "    p = number of Independent Feature\n",
    "    \n",
    "\n",
    "Difference between Adjusted R-Squared and R-Squared:\n",
    "    \n",
    "    The key difference between adjusted R-squared and R-squared is that the former takes into account the number of independent variables in the model. As the number of independent variables increases, R-squared will generally increase as well, even if the additional variables do not improve the model's performance. Adjusted R-squared, on the other hand, will only increase if the additional variables improve the model's performance beyond what would be expected by chance. In this way, adjusted R-squared provides a more reliable measure of the goodness of fit of a regression model, particularly when comparing models with different numbers of independent variables.\n",
    "    \n",
    "    Adjusted R-squared is a modified version of the R-squared statistic that takes into account the number of independent variables in a regression model. It provides a more reliable measure of the goodness of fit of the model, particularly when comparing models with different numbers of independent variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0421a0ad-b104-4ed2-8dac-118a3a7e3bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "568bd65b-3941-496c-bf66-176fef1510bd",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "ans:\n",
    "    \n",
    "    When the number of independent variables is large, R-squared tends to increase even if the additional variables do not improve the model's performance. Adjusted R-squared, on the other hand, takes into account the number of independent variables and only increases if the additional variables improve the model's performance beyond what would be expected by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5d47f-5aae-4e14-8b33-864049fc9d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dbd8105-e4f3-4aac-8e4e-587883ae6849",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n",
    "\n",
    "ans:\n",
    "    \n",
    "RMSE:\n",
    "    RMSE is the square root of the average of the squared differences between the predicted values and the actual values. \n",
    "    \n",
    "    RMSE = sqrt(1/n * Σ(y_pred - y_actual)^2)\n",
    "\n",
    "    where n is the number of observations, y_pred is the predicted value, and y_actual is the actual value.\n",
    "\n",
    "MSE:\n",
    "    MSE is the average of the squared differences between the predicted values and the actual values. \n",
    "\n",
    "    MSE = 1/n * Σ(y_pred - y_actual)^2\n",
    "\n",
    "    where n is the number of observations, y_pred is the predicted value, and y_actual is the actual value.\n",
    "\n",
    "MAE:\n",
    "    MAE is the average of the absolute differences between the predicted values and the actual values.\n",
    "\n",
    "    MAE = 1/n * Σ|y_pred - y_actual|\n",
    "\n",
    "    where n is the number of observations, y_pred is the predicted value, and y_actual is the actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89581fbd-9a6e-4aad-a223-5cc357415e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14be1722-777a-4a1e-b985-0f90e6156372",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n",
    "\n",
    "ans:\n",
    "    \n",
    "MSE :     \n",
    "\n",
    "    Advantage: \n",
    "        1. the equation is Quadratic, hence it can be differentiable. \n",
    "        2. It has only one Global Minima\n",
    "\n",
    "    Disadvantage: \n",
    "        1. not robust to Outliers.\n",
    "        2. unit will be squared\n",
    "\n",
    "\n",
    "MAE:\n",
    "    \n",
    "    Advantage: \n",
    "        1. Robust to Outliers\n",
    "        2. Unit is same\n",
    "        \n",
    "    Disadvantage: \n",
    "        1. Convergence takes time\n",
    "        2. Optimization is complex\n",
    "        \n",
    "\n",
    "RMSE: \n",
    "    \n",
    "    Advantage: \n",
    "        1. it can be differentiable. \n",
    "        2. Unit Remain same\n",
    "        \n",
    "    Disadvantage: \n",
    "        1. not robust to Outliers.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ce180-7ed7-45f5-8cd2-d2cd9786ad59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83aa9e04-87b4-443f-8c62-79cf93e925c9",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\n",
    "\n",
    "ans: \n",
    "    \n",
    "Lasso Regularization:\n",
    "    \n",
    "    Lasso regularization, also known as L1 regularization, is a technique used in linear regression to prevent overfitting by adding a penalty term to the cost function. The penalty term is the absolute value of the coefficients of the features, multiplied by a regularization parameter alpha.\n",
    "\n",
    "Ridge Regularization:\n",
    "\n",
    "    Ridge regularization, also known as L2 regularization, adds a penalty term to the cost function that is proportional to the square of the coefficients of the features. This method does not lead to feature selection, but instead shrinks the coefficients of all the features towards zero, reducing their impact on the model.\n",
    "    \n",
    "use case of Lasso and Ridge Regularization:\n",
    "\n",
    "    The choice between Lasso and Ridge regularization depends on the nature of the data and the problem being solved. If there are a large number of features, but only a few of them are important for the model, then Lasso regularization may be more appropriate as it will result in a sparse model. On the other hand, if all the features are important and the goal is to reduce their impact rather than eliminate them, then Ridge regularization may be more appropriate.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b86a35-47b6-4bb8-943e-810fd14c9155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d3186ae-24a0-49f7-8cba-e50201b9c84e",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.\n",
    "\n",
    "ans: \n",
    "    \n",
    "Regularized linear models help prevent overfitting in machine learning by adding a penalty term to the cost function that penalizes large values of the model's coefficients. This penalty term reduces the model's complexity and forces it to generalize better to new data. Regularization is especially useful when dealing with high-dimensional data, where the number of features is large compared to the number of training examples.\n",
    "\n",
    "Here's an example to illustrate the concept:\n",
    "\n",
    "Suppose we want to predict the price of a house based on its size, number of bedrooms, and location. We have a dataset of 1000 houses with their corresponding prices, and we want to build a linear regression model to predict the price of a new house based on its features.\n",
    "\n",
    "We first fit a linear regression model to the data without regularization. This model may overfit the training data if there are too many features or if the features are highly correlated. To prevent overfitting, we can use regularized linear models such as Ridge regression or Lasso regression.\n",
    "\n",
    "For example, we can use Lasso regression to fit the data, which adds a penalty term to the cost function that is proportional to the absolute value of the coefficients of the features. This penalty term encourages sparsity by shrinking the coefficients of the less important features towards zero. As a result, Lasso regression selects a subset of the most important features, effectively reducing the dimensionality of the problem and preventing overfitting.\n",
    "\n",
    "In this example, Lasso regression helps prevent overfitting by selecting the most important features for predicting house prices and shrinking the coefficients of the less important features. This results in a more interpretable model that is less likely to overfit the training data and generalize better to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c1128-2886-4696-b8fb-96792723e8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8038645-781e-4d81-a790-2a6437251414",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n",
    "\n",
    "ans:\n",
    "    \n",
    "Although regularized linear models such as Ridge regression and Lasso regression are effective at preventing overfitting in machine learning, they may not always be the best choice for regression analysis due to their limitations:\n",
    "\n",
    "Non-linear relationships:\n",
    "        Regularized linear models assume a linear relationship between the features and the target variable. However, in many real-world scenarios, the relationship between the features and the target variable may be non-linear. In such cases, other regression techniques such as polynomial regression or decision trees may be more appropriate.\n",
    "\n",
    "Feature scaling:\n",
    "    Regularized linear models are sensitive to the scale of the features. If the features are not properly scaled, the regularization may not work as intended, and the model may perform poorly. Scaling the features before fitting the model can mitigate this issue, but it can be computationally expensive for large datasets.\n",
    "\n",
    "Hyperparameter tuning: \n",
    "    Regularized linear models have hyperparameters that need to be tuned to obtain optimal performance. Tuning these hyperparameters can be time-consuming and requires domain expertise. Moreover, the optimal hyperparameters may vary depending on the dataset and the problem being solved.\n",
    "\n",
    "Interpretability: \n",
    "    Regularized linear models can be less interpretable than other regression techniques. The penalty term added to the cost function can make it harder to understand the relationship between the features and the target variable. In contrast, simpler models such as linear regression or decision trees may be easier to interpret.\n",
    "\n",
    "In summary, regularized linear models can be a powerful tool for preventing overfitting in regression analysis. However, they may not always be the best choice, especially when dealing with non-linear relationships, unscaled features, or the need for interpretability. Choosing the appropriate regression technique depends on the nature of the data and the problem being solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7740671-2da8-4eb8-a38a-391e06b420a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e35b39f0-7b7c-4e35-a948-4e10ef44e5b9",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "ans:\n",
    "\n",
    "    If the problem requires predicting values that are sensitive to large errors, then RMSE may be a more appropriate metric as it penalizes larger errors more heavily. In this case, Model A with an RMSE of 10 would be the better performer.\n",
    "\n",
    "    However, if the problem requires predicting values where small errors are acceptable, then MAE may be more appropriate as it is less sensitive to outliers. In this case, Model B with an MAE of 8 would be the better performer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee58515-364f-47be-aa84-b66fef021117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c633811c-4512-4d38-838a-69563565c201",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?\n",
    "\n",
    "ans:\n",
    "    \n",
    "If the problem requires a smaller set of features, then Lasso regularization may be more appropriate as it can perform feature selection. In this case, Model B with a regularization parameter of 0.5 may be the better performer. \n",
    "\n",
    "However, if the goal is to keep all features and only penalize large coefficients, then Ridge regularization may be more appropriate. In this case, Model A with a regularization parameter of 0.1 may be the better performer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f804ccc5-7f84-47fd-b72e-4d14d51dc6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
