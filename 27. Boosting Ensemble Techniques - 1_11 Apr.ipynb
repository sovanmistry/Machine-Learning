{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4879bdca-c3b2-49e0-8ae4-295a7d58f460",
   "metadata": {},
   "source": [
    "## 11 Apr Ensemble Techniques - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1848af12-5e43-4ba6-9d59-47516bbf4e6f",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?\n",
    "\n",
    "Ans: \n",
    "    \n",
    "\n",
    "    An ensemble technique in machine learning is a method that combines multiple models to improve the accuracy and robustness of the predictions. The basic idea behind ensemble methods is to take multiple models that may have different strengths and weaknesses, and combine their predictions to create a more accurate and reliable prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8125206a-997f-4e81-b71d-9dca46548511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2445b2a-2b60-4144-bc53-6e51f5e790b4",
   "metadata": {},
   "source": [
    "Q2. Why are ensemble techniques used in machine learning?\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    1. Bagging (Bootstrap Aggregating): This involves training multiple models on different subsets of the training data and averaging their predictions to create the final prediction. Bagging is commonly used with decision trees.\n",
    "\n",
    "    2. Boosting: Boosting involves training multiple weak models sequentially, where each subsequent model is trained to correct the errors of the previous model. The final prediction is a weighted average of the individual model predictions.\n",
    "\n",
    "    3. Stacking: Stacking involves training multiple models and using their predictions as input features for a meta-model that makes the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc656e-deca-48f0-a1bd-bb2623310531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "640a3d78-c24e-40e0-a6ad-60a00891a19f",
   "metadata": {},
   "source": [
    "Q3. What is bagging?\n",
    "\n",
    "Ans: \n",
    "    \n",
    "    Bagging, short for \"Bootstrap Aggregating,\" is an ensemble technique in machine learning that involves training multiple models on different subsets of the training data and combining their predictions to create a more accurate and robust prediction. The basic idea behind bagging is to reduce the variance of the model by introducing randomness in the training process.\n",
    "\n",
    "    Here's how bagging works:\n",
    "\n",
    "    Given a training dataset with N examples, multiple subsets (or \"bags\") of size n are created by randomly sampling with replacement from the original dataset. This means that some examples may appear multiple times in a single bag, while others may not appear at all.\n",
    "\n",
    "    A separate model is trained on each bag. The models are typically of the same type and are trained using the same algorithm.\n",
    "\n",
    "    Once all the models have been trained, they are combined to make a final prediction. For regression problems, this can be done by averaging the individual model predictions, while for classification problems, the final prediction is made by taking a majority vote among the individual model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ae254-b5ae-4917-b68c-84fcbb66704b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f306d0b6-094a-434d-845d-be85a3152c17",
   "metadata": {},
   "source": [
    "Q4. What is boosting?\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    Boosting is an ensemble technique in machine learning that involves training multiple weak models sequentially, where each subsequent model is trained to correct the errors of the previous model. The basic idea behind boosting is to improve the accuracy of the model by focusing on the examples that are difficult to classify correctly.\n",
    "\n",
    "    Here's how boosting works:\n",
    "\n",
    "    1. A weak model is trained on the entire training dataset.\n",
    "\n",
    "    2. The examples that were misclassified by the first model are given greater weight, while the examples that were correctly classified are given lower weight.\n",
    "\n",
    "    3. A second weak model is trained on the modified dataset, where the misclassified examples are given higher weight. The second model is trained to focus on the examples that were difficult for the first model to classify correctly.\n",
    "\n",
    "    4. The process is repeated for multiple iterations, where each subsequent model is trained to correct the errors of the previous models.\n",
    "\n",
    "    5. The final prediction is made by combining the predictions of all the individual models. For regression problems, this can be done by averaging the individual model predictions, while for classification problems, the final prediction is made by taking a weighted vote among the individual model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce50c25-ab6f-4fea-a5e7-61db77fefb67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4fad322-ab7f-4944-b179-ff5a8e683dde",
   "metadata": {},
   "source": [
    "Q5. What are the benefits of using ensemble techniques?\n",
    "\n",
    "Ans: \n",
    "    \n",
    "    \n",
    "    There are several benefits of using ensemble techniques in machine learning, including:\n",
    "\n",
    "    Improved Accuracy: Ensemble techniques can often improve the accuracy and robustness of machine learning models by combining the strengths of multiple models and reducing the impact of their weaknesses. By creating a more accurate and reliable prediction, ensemble techniques can help to improve the performance of the model on both the training and testing datasets.\n",
    "\n",
    "    Reduced Overfitting: Ensemble techniques can help to reduce the risk of overfitting by introducing randomness and diversity into the model training process. By creating multiple models that are trained on different subsets of the training data, ensemble techniques can help to prevent the model from memorizing the training data and improve its generalization performance on new, unseen data.\n",
    "\n",
    "    Better Handling of Noise and Outliers: Ensemble techniques can help to improve the model's ability to handle noisy or outlier data points by reducing their impact on the final prediction. By creating multiple models that are trained on different subsets of the training data, ensemble techniques can help to identify and remove the impact of noisy or outlier data points that might otherwise skew the final prediction.\n",
    "\n",
    "    Flexibility and Adaptability: Ensemble techniques can be applied to a wide range of machine learning problems and can be used with any type of model or algorithm. Ensemble techniques can also be adapted to different datasets and problem domains, making them a flexible and adaptable tool for machine learning practitioners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5438f61c-2faf-44b4-9c26-819f83441086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6fc8b51-8d06-479f-92a4-ac677f8931e5",
   "metadata": {},
   "source": [
    "Q6. Are ensemble techniques always better than individual models?\n",
    "\n",
    "Ans:\n",
    "\n",
    "    Ensemble techniques are not always better than individual models, and there may be cases where an individual model performs better than an ensemble of models. However, in many cases, ensemble techniques can improve the performance of the model by combining the strengths of multiple models and reducing the impact of their weaknesses.\n",
    "\n",
    "    The effectiveness of ensemble techniques depends on several factors, including the quality and diversity of the individual models, the size and complexity of the dataset, and the problem domain. In some cases, the dataset may be small or the individual models may be very accurate on their own, in which case an ensemble may not provide significant improvements. On the other hand, for larger and more complex datasets, an ensemble of models can often help to improve the model's accuracy, robustness, and generalization performance.\n",
    "\n",
    "    It is also worth noting that ensemble techniques can be computationally expensive and may require more resources and time than training a single model. Therefore, the decision to use an ensemble technique should be based on a careful evaluation of the benefits and costs, and should be tailored to the specific problem and dataset at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba4abfe-d93a-4326-b8a4-0668cbaac333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e3b8ed8-79c6-4260-a14d-ecb02108721b",
   "metadata": {},
   "source": [
    "Q7. How is the confidence interval calculated using bootstrap?\n",
    "\n",
    "Ans: \n",
    "    \n",
    "    1. Randomly sample the original dataset with replacement to create a new dataset of the same size.\n",
    "\n",
    "    2. Calculate the parameter or performance metric of interest on the new dataset.\n",
    "\n",
    "    3. Repeat steps 1 and 2 many times (e.g., 1,000 or 10,000 times) to create a distribution of parameter or metric estimates.\n",
    "\n",
    "    4. Calculate the lower and upper bounds of the confidence interval by finding the percentiles of the distribution that correspond to the desired confidence level. For example, a 95% confidence interval can be calculated by finding the 2.5th and 97.5th percentiles of the distribution.\n",
    "\n",
    "    The resulting confidence interval provides a range of plausible values for the true parameter or metric, based on the observed variation in the resampled datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408ab8a-7851-4d6f-a5d4-5e30453e0f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e6db3f6-ed17-4979-a7ee-e8a78c19c973",
   "metadata": {},
   "source": [
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    1. Collect the original dataset: Collect the original dataset that contains the observations or samples for which we want to estimate the parameter or metric.\n",
    "\n",
    "    2. Sample with replacement: Randomly sample the original dataset with replacement to create a new dataset of the same size. This resampling process ensures that each observation has an equal chance of being selected multiple times or not at all.\n",
    "\n",
    "    3. Calculate the parameter/metric of interest: Calculate the parameter or metric of interest on the new dataset. For example, we might calculate the mean, variance, or correlation coefficient for a statistical parameter, or accuracy, precision, recall, or F1-score for a machine learning model's performance metric.\n",
    "\n",
    "    4. Repeat the resampling process: Repeat steps 2 and 3 many times (e.g., 1,000 or 10,000 times) to create a distribution of parameter or metric estimates.\n",
    "\n",
    "    5. Calculate the confidence interval: Calculate the lower and upper bounds of the confidence interval by finding the percentiles of the distribution that correspond to the desired confidence level. For example, a 95% confidence interval can be calculated by finding the 2.5th and 97.5th percentiles of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdcc1f9-58c1-4d4f-9cce-240c2c1bce82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5878959-9cfe-41ed-8f73-561a995afa16",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a \n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use \n",
    "bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa50febd-da1a-44f6-90b6-ed93759a9eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for Mean Height of Trees (m):\n",
      "Lower Bound: 14.451058711020538\n",
      "Upper Bound: 15.551801984424223\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# original sample data\n",
    "sample_mean = 15\n",
    "sample_std = 2\n",
    "n = 50\n",
    "\n",
    "# create array to store bootstrap means\n",
    "bootstrap_means = np.zeros(10000)\n",
    "\n",
    "# bootstrap resampling\n",
    "for i in range(10000):\n",
    "    # resample with replacement\n",
    "    resample = np.random.normal(sample_mean, sample_std, n)\n",
    "    # calculate mean of resample\n",
    "    bootstrap_means[i] = np.mean(resample)\n",
    "\n",
    "# calculate confidence interval\n",
    "ci_lower = np.percentile(bootstrap_means, 2.5)\n",
    "ci_upper = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "print(\"95% Confidence Interval for Mean Height of Trees (m):\")\n",
    "print(\"Lower Bound:\", ci_lower)\n",
    "print(\"Upper Bound:\", ci_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3276103b-d596-46d1-944c-3ed123f382a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
