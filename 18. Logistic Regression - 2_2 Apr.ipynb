{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53ffd087-ee94-48cc-b12a-786b1966dd55",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression - 2_2 Apr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a2823-3ed0-4e31-af17-34c7ce3f22af",
   "metadata": {},
   "source": [
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Grid search cross-validation (GridSearchCV) is a technique used in machine learning to optimize the hyperparameters of a model. Hyperparameters are parameters that are not learned from the data but are set by the user before model training. Examples of hyperparameters include the learning rate, regularization strength, and number of hidden layers in a neural network.\n",
    "\n",
    "The purpose of GridSearchCV is to search over a specified range of hyperparameter values and find the combination of values that gives the best performance on a given performance metric, such as accuracy, precision, or recall. GridSearchCV does this by performing a cross-validation procedure on each combination of hyperparameter values in a grid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d74d7-2caa-44ac-9480-10f465c4ea48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f9a3910-6f64-4f61-824f-ba8e333b3f7c",
   "metadata": {},
   "source": [
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "one over the other?\n",
    "\n",
    "Ans:\n",
    "Difference between grid search cv and randomize search cv:\n",
    "\n",
    "1. Search space: Grid search explores a predefined set of hyperparameters, with each combination of hyperparameters tested in a grid-like fashion. Random search, on the other hand, randomly samples hyperparameters from a distribution defined by the user.\n",
    "\n",
    "2. Computational cost: Grid search can be computationally expensive when the number of hyperparameters and their values are large. Random search, on the other hand, is more computationally efficient as it explores a smaller portion of the hyperparameter space.\n",
    "\n",
    "3. Exploration vs. exploitation: Grid search is good at exploitation, meaning it thoroughly searches the hyperparameter space to find the optimal combination of hyperparameters. Random search, on the other hand, is better at exploration as it randomly samples the hyperparameter space, which can be useful when the optimal hyperparameters are not well-defined.\n",
    "\n",
    "use case between grid search cv and randomize search cv:\n",
    "\n",
    "1. Grid search is better suited for smaller hyperparameter spaces, where it is feasible to test all possible combinations of hyperparameters.\n",
    "\n",
    "2. Random search is better suited for larger hyperparameter spaces, where testing all possible combinations would be computationally expensive or impractical.\n",
    "\n",
    "3. If there are some hyperparameters that are known to be more important than others, grid search can be a good option as it focuses on these hyperparameters. Random search can be useful when there is no prior knowledge of which hyperparameters are important.\n",
    "\n",
    "4. If the hyperparameters are evenly distributed in the search space, both grid search and random search can be equally effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3669585f-8dae-4918-9fed-123f24d8a214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da6debde-c12c-47a6-a89e-975b822d039f",
   "metadata": {},
   "source": [
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.\n",
    "\n",
    "Ans:\n",
    "    \n",
    "Data leakage occurs when the model is trained using information that would not be available at the time of prediction. This can happen in several ways, such as:\n",
    "\n",
    "Target leakage: When the training dataset includes information that is directly related to the target variable, but would not be available at the time of prediction. For example, if a credit risk model uses the customer's current credit score to predict whether they will default on their loan, this would be considered target leakage because the current credit score would not be available at the time the loan was issued.\n",
    "\n",
    "Train-test contamination: When the training dataset is contaminated with information from the test dataset, leading to overfitting. For example, if the test dataset includes data from a future time period, and this data is included in the training dataset, the model may learn to use this future information to make predictions, leading to overly optimistic performance estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e291a61-9da8-49f0-960d-a79c6fd25d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bcc451e-d9f0-4792-929c-9d8cea0685c4",
   "metadata": {},
   "source": [
    "Q4. How can you prevent data leakage when building a machine learning model?\n",
    "\n",
    "Ans:\n",
    "    \n",
    "To prevent data leakage when building a machine learning model, you can follow these best practices:\n",
    "\n",
    "1. Use a separate dataset for model evaluation: Keep a separate dataset for final model evaluation, which is not used during model development or hyperparameter tuning. This ensures that the model is evaluated on data that is truly independent of the training data.\n",
    "\n",
    "2. Use cross-validation for hyperparameter tuning: Use cross-validation instead of a single train-test split to evaluate the performance of the model during hyperparameter tuning. This helps prevent overfitting to the validation set.\n",
    "\n",
    "3. Carefully preprocess the data: Preprocess the data carefully to avoid including information that would not be available at the time of prediction. For example, if you are predicting future stock prices, do not include information about the future stock prices in the training data.\n",
    "\n",
    "4. Remove highly correlated features: Remove highly correlated features that could lead to data leakage. For example, if you are predicting whether a customer will buy a product, do not include the customer's purchase history in the training data.\n",
    "\n",
    "5. Use time-series cross-validation for time-series data: If you are working with time-series data, use time-series cross-validation to ensure that the model is not overfitting to future data.\n",
    "\n",
    "6. Regularly monitor the model in production: Regularly monitor the model in production to ensure that it is not using information that would not be available at the time of prediction. For example, if you are predicting credit risk, regularly monitor the model to ensure that it is not using the customer's current credit score, which would not be available at the time the loan was issued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da907499-8481-451e-9679-45758af994d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "999c585a-e164-4e41-8a2d-f1c0240518b3",
   "metadata": {},
   "source": [
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?\n",
    "\n",
    "Ans: \n",
    "    \n",
    "A confusion matrix is a table that summarizes the performance of a classification model by comparing the predicted labels with the actual labels. It is a matrix that consists of four terms:\n",
    "\n",
    "True Positive (TP): The number of instances that are correctly predicted as positive (belonging to the positive class).\n",
    "False Positive (FP): The number of instances that are incorrectly predicted as positive (not belonging to the positive class).\n",
    "True Negative (TN): The number of instances that are correctly predicted as negative (not belonging to the positive class).\n",
    "False Negative (FN): The number of instances that are incorrectly predicted as negative (belonging to the positive class).\n",
    "\n",
    "A confusion matrix helps to evaluate the performance of a classification model by providing insight into the types of errors that the model is making. By comparing the predicted labels with the actual labels, we can calculate several metrics such as:\n",
    " \n",
    " 1. Accuracy : \n",
    " (TP+TN)/(TP+TN+FP+FN)\n",
    " \n",
    " 2. Precision:\n",
    " TP/(TP+FP)\n",
    " \n",
    " 3. Recall :\n",
    " TP/(TP+FN)\n",
    " \n",
    " 4. FBETA SCORE:\n",
    " (1+beta*beta) (Precision*Recall)/Precision+Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a76de4-603d-4377-8f4c-dd5f9156e96c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e02a2cf8-5fd0-4bd6-aa46-ecb40899beee",
   "metadata": {},
   "source": [
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix.\n",
    "\n",
    "Ans:\n",
    "    \n",
    "Precision is the proportion of true positive predictions (TP) out of all positive predictions (TP + FP). It measures how many of the predicted positive instances are actually positive.\n",
    "\n",
    "Precision: TP/(TP+FP)\n",
    "\n",
    "Recall, also known as sensitivity, is the proportion of true positive predictions (TP) out of all actual positive instances (TP + FN). It measures how many of the actual positive instances were correctly predicted.\n",
    "\n",
    "\n",
    "Recall : TP/(TP+FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c59b97b-c955-44fd-89c6-8745a8a29146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6a22005-3815-4b3c-846a-a64f88eea537",
   "metadata": {},
   "source": [
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?\n",
    "\n",
    "Ans:\n",
    "    \n",
    "A confusion matrix is a table that summarizes the performance of a classification model by comparing the predicted labels with the true labels. The confusion matrix consists of four values: true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN).\n",
    "\n",
    "By looking at these values, you can identify which types of errors your model is making. For example:\n",
    "\n",
    "1. If you have a high number of false positives, your model may be too sensitive to the positive class, and is predicting too many instances to belong to the positive class when they should not. This could be the case if you have a highly imbalanced dataset with a low proportion of positive instances, and your model is overfitting to the majority class.\n",
    "\n",
    "2. If you have a high number of false negatives, your model may be too conservative and is missing many positive instances that should be classified as such. This could be the case if you have a highly imbalanced dataset with a high proportion of positive instances, and your model is biased towards the majority class.\n",
    "\n",
    "3. If you have a high number of true positives and true negatives, but low values of false positives and false negatives, your model is performing well overall and is making accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8300cdee-bd18-4dc3-b031-21d6a7670130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b19e7d7-3770-45b4-8478-0ced8da0d344",
   "metadata": {},
   "source": [
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "calculated?\n",
    "\n",
    "Ans:\n",
    "    \n",
    " \n",
    " 1. Accuracy : \n",
    " (TP+TN)/(TP+TN+FP+FN)\n",
    " \n",
    " 2. Precision:\n",
    " TP/(TP+FP)\n",
    " \n",
    " 3. Recall :\n",
    " TP/(TP+FN)\n",
    " \n",
    " 4. FBETA SCORE:\n",
    " (1+beta*beta) (Precision*Recall)/Precision+Recall)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf49ecc2-eb96-4af6-b1d4-e7fe0a116261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ca1e31f-fb51-405e-ab53-6e2b5213346f",
   "metadata": {},
   "source": [
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?\n",
    "\n",
    "Ans: \n",
    "    \n",
    "Accuracy : (TP+TN)/(TP+TN+FP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550433f-dc1f-4d16-9e61-84136c1e3aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c25f9-e790-4f8a-8e31-e755db58b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning\n",
    "model?\n",
    "\n",
    "Ans:\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
