{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d635dd-2ab4-4b64-8066-5ef35915e298",
   "metadata": {},
   "source": [
    "                                            16 March Assignment Machine Learning 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbf9a15-4ca8-44de-93b2-f19c58e007b0",
   "metadata": {},
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb29704-b32e-4fbc-95c6-2cc204c1c917",
   "metadata": {},
   "source": [
    "ans: \n",
    "    \n",
    "    Overfitting : \n",
    "        when ML model trained with so much data and it learns from the inaccurate data then it gives wrong result. \n",
    "        \n",
    "    Underfitting : \n",
    "        the model which neither perform well in the train dataset nor on test dataset.\n",
    "        \n",
    "    Mitigation process:\n",
    "        \n",
    "        Overfitting : we have to remove inaccurate data.\n",
    "        Underfitting: we have to train our model with sufficient and good amount of data.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff5802-f9b8-487a-9db9-fbfdfc41c84f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61c90e32-8cef-4cfb-a854-01d6302bf3ec",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief.\n",
    "\n",
    "ans: \n",
    "    \n",
    "    \n",
    "1. Train with more data\n",
    "\n",
    "With the increase in the training data, the crucial features to be extracted become prominent. The model can recognize the relationship between the input attributes and the output variable. The only assumption in this method is that the data to be fed into the model should be clean; otherwise, it would worsen the problem of overfitting.\n",
    "\n",
    "2. Data augmentation\n",
    "\n",
    "An alternative method to training with more data is data augmentation, which is less expensive and safer than the previous method. Data augmentation makes a sample data look slightly different every time the model processes it. \n",
    "\n",
    "3. Addition of noise to the input data \n",
    "\n",
    "Another similar option as data augmentation is adding noise to the input and output data. Adding noise to the input makes the model stable without affecting data quality and privacy while adding noise to the output makes the data more diverse. Noise addition should be done in limit so that it does not make the data incorrect or too different.\n",
    "\n",
    "4. Feature selection\n",
    "\n",
    "Every model has several parameters or features depending upon the number of layers, number of neurons, etc.  The model can detect many redundant features or features determinable from other features leading to unnecessary complexity. We very well know that the more complex the model, the higher the chances of the model to overfit. \n",
    "\n",
    "5. Cross-validation\n",
    "\n",
    "Cross-validation is a robust measure to prevent overfitting. The complete dataset is split into parts. In standard K-fold cross-validation, we need to partition the data into k folds. Then, we iteratively train the algorithm on k-1 folds while using the remaining holdout fold as the test set. This method allows us to tune the hyperparameters of the neural network or machine learning model and test it using completely unseen data. \n",
    "\n",
    "6. Simplify data\n",
    "\n",
    "Till now, we have come across model complexity to be one of the top reasons for overfitting. The data simplification method is used to reduce overfitting by decreasing the complexity of the model to make it simple enough that it does not overfit. Some of the procedures include pruning a decision tree, reducing the number of parameters in a neural network, and using dropout on a neutral network. \n",
    "\n",
    "7. Regularization\n",
    "\n",
    "If overfitting occurs when a model is too complex, reducing the number of features makes sense. Regularization methods like Lasso, L1 can be beneficial if we do not know which features to remove from our model. Regularization applies a \"penalty\" to the input parameters with the larger coefficients, which subsequently limits the model's variance. \n",
    "\n",
    "8. Ensembling\n",
    "\n",
    "It is a machine learning technique that combines several base models to produce one optimal predictive model. In Ensemble learning,  the predictions are aggregated to identify the most popular result. Well-known ensemble methods include bagging and boosting, which prevents overfitting as an ensemble model is made from the aggregation of multiple models. \n",
    "\n",
    "\n",
    "9. Early stopping\n",
    "\n",
    "This method aims to pause the model's training before memorizing noise and random fluctuations from the data. There can be a risk that the model stops training too soon, leading to underfitting. One has to come to an optimum time/iterations the model should train. \n",
    "\n",
    "10. Adding dropout layers\n",
    "\n",
    "Large weights in a neural network signify a more complex network. Probabilistically dropping out nodes in the network is a simple and effective method to prevent overfitting. In regularization, some number of layer outputs are randomly ignored or “dropped out” to reduce the complexity of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50f73c3-b383-44b0-a4d1-afb589ce412e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cb32d91-29b3-4bda-9c91-1668fc8ae0c3",
   "metadata": {},
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "\n",
    "ans: \n",
    "    \n",
    "    Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data\n",
    "    \n",
    "    \n",
    "Senarios of Underfitting: \n",
    "\n",
    "    High bias and low variance \n",
    "    The size of the training dataset used is not enough.\n",
    "    The model is too simple.\n",
    "    Training data is not cleaned and also contains noise in it.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0673f94-e835-49e3-aa4d-dee24fc77707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eff2b3c7-ed21-47fb-a683-febf2c163d03",
   "metadata": {},
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?\n",
    "\n",
    "\n",
    "ans: \n",
    "    \n",
    "Bias : Error rate on training data compare with our prediction.\n",
    "\n",
    "Variance: Error rate on training data compare with the Test Data.\n",
    "\n",
    "Overfitting : Low bias high Variance\n",
    "\n",
    "Underfitting: high bias low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3601d135-ad43-45fb-abd2-35845cf7dfcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef3c9274-7ffa-40da-aef9-3373384ce070",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?\n",
    "\n",
    "ans:\n",
    "    \n",
    "Overfitting:\n",
    "\n",
    "Your model is overfitting your training data when you see that the model performs well on the training data but does not perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples.\n",
    "    \n",
    "Underfitting : \n",
    "\n",
    "Your model is underfitting the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples (often called X) and the target values (often called Y). \n",
    "\n",
    "Overfitting : Low bias high Variance\n",
    "\n",
    "Underfitting: high bias low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a0dd5-c626-4e6f-a46f-c568bd6d45d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfd98cbe-41c3-4a1e-b433-e99298fd57ad",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?\n",
    "\n",
    "ans: \n",
    "    \n",
    "Bias : Error rate on training data compare with our prediction.\n",
    "\n",
    "Variance: Error rate on training data compare with the Test Data.\n",
    "\n",
    "High Bias Model: Suppose I read chapter 1 and going to valdate with by answering questions of chapter 2\n",
    "\n",
    "High Variance Model : out of 10 chapters, I read it only 1 chapter, on validation test I will fail and on the real test also I will fail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b3835-1e2d-4106-b8f6-91aedf8697a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "249d4cfe-38e1-4545-b093-f342a5fc9782",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work.\n",
    "\n",
    "ans:\n",
    "    \n",
    "\n",
    "If overfitting occurs when a model is too complex, reducing the number of features makes sense. Regularization methods like Lasso, L1 can be beneficial if we do not know which features to remove from our model. Regularization applies a \"penalty\" to the input parameters with the larger coefficients, which subsequently limits the model's variance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6eff51-4527-4092-8282-f04d408867b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
