{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6abf6492-7c53-4d19-acf0-001575e6e58d",
   "metadata": {},
   "source": [
    "## 20 Apr KNN - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f541d-4c27-4aa2-9555-ac4a088023cd",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    The K-Nearest Neighbors (KNN) algorithm is a non-parametric classification or regression technique in machine learning. It is used for predicting the class of a new observation by finding the K nearest data points from the training set and classifying the new observation based on the majority class of its K nearest neighbors.\n",
    "\n",
    "    In the case of classification, the KNN algorithm assigns a class label to a new observation based on the class labels of its K nearest neighbors. In the case of regression, the KNN algorithm predicts the value of a new observation by taking the average of the values of its K nearest neighbors.\n",
    "\n",
    "    The value of K is a hyperparameter that determines the number of neighbors to consider when making a prediction. It is typically chosen by cross-validation, where the performance of the algorithm is evaluated on a validation set for different values of K, and the value of K that gives the best performance is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd44a947-4f33-479a-a113-8bb413f49234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "093f68e9-c162-4977-bd32-202dd9c5ee06",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    The choice of the value of K in KNN algorithm is a crucial step as it determines the number of nearest neighbors used to classify a new data point. The value of K can greatly affect the accuracy and performance of the KNN algorithm. A small value of K may lead to overfitting, while a large value of K may lead to underfitting.\n",
    "\n",
    "    Here are some common methods for selecting the optimal value of K:\n",
    "\n",
    "    Grid Search: In this method, we evaluate the performance of the KNN algorithm for different values of K using a validation set. The optimal value of K is chosen based on the performance metric (such as accuracy) that gives the best result.\n",
    "\n",
    "    Cross-validation: In this method, the data is partitioned into several folds, and the KNN algorithm is trained and tested on different subsets of data. The optimal value of K is chosen based on the performance metric that gives the best result.\n",
    "\n",
    "    Domain Knowledge: In some cases, the choice of K may depend on the domain knowledge of the problem. For example, in a medical diagnosis problem, the choice of K may depend on the number of similar cases observed in the past.\n",
    "\n",
    "    Rule of Thumb: A common rule of thumb is to choose the value of K as the square root of the number of data points in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e1dea-53a4-4e01-b965-bcf74051c9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a67ed5f-e3c6-4478-b9ef-96e30ea56e93",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "Ans: \n",
    "    \n",
    "    In KNN classifier, the output is a class label, which is assigned to a new observation based on the class labels of its K nearest neighbors. The KNN classifier is used for classification tasks, where the goal is to predict the class label of a new observation based on its features.\n",
    "\n",
    "    On the other hand, in KNN regressor, the output is a continuous numerical value, which is predicted based on the average of the values of its K nearest neighbors. The KNN regressor is used for regression tasks, where the goal is to predict a continuous numerical value for a new observation based on its features.\n",
    "\n",
    "    Another difference between KNN classifier and KNN regressor is the performance metric used to evaluate their performance. In KNN classifier, the most common performance metrics are accuracy, precision, recall, F1-score, and confusion matrix. In KNN regressor, the most common performance metrics are mean squared error, mean absolute error, and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb4e96-92fb-4049-8c88-6551029b8c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abbfe0a4-8272-42c3-aa5d-4e9b0bc32a5a",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    The performance of KNN can be measured using various metrics depending on the type of problem (classification or regression) and the evaluation goals. Here are some common performance metrics used for KNN:\n",
    "\n",
    "    1. Classification Metrics:\n",
    "    \n",
    "        Accuracy: It is the most commonly used metric for evaluating the performance of KNN classification. It measures the proportion of correctly classified samples.\n",
    "\n",
    "        Precision: It measures the proportion of correctly classified positive samples out of all the predicted positive samples.\n",
    "\n",
    "        Recall: It measures the proportion of correctly classified positive samples out of all the actual positive samples.\n",
    "\n",
    "        F1-score: It is a harmonic mean of precision and recall, and provides a balance between these two metrics.\n",
    "\n",
    "        Confusion Matrix: It is a matrix that shows the true positives, true negatives, false positives, and false negatives.\n",
    "    \n",
    "    2. Regression Metrics:\n",
    "    \n",
    "        Mean Squared Error (MSE): It measures the average of the squared differences between the predicted and actual values.\n",
    "\n",
    "        Mean Absolute Error (MAE): It measures the average of the absolute differences between the predicted and actual values.\n",
    "\n",
    "        R-squared (R2): It measures the proportion of the variance in the target variable that is explained by the model.\n",
    "\n",
    "    3. Cross-validation:\n",
    "\n",
    "        Cross-validation is a technique used to estimate the generalization performance of KNN on new data. It involves partitioning the data into several subsets, training the model on one subset, and testing on another subset. The performance is then averaged over all the subsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb84c7e0-b4d8-4259-803f-8db3b60fc6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df06f3b0-26c9-4866-9cf3-c063070235c4",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    The curse of dimensionality is a phenomenon that occurs when working with high-dimensional data in machine learning algorithms such as KNN. In the context of KNN, the curse of dimensionality refers to the problem of increasing the number of features or dimensions in the data, which can significantly affect the performance of the algorithm.\n",
    "\n",
    "    The curse of dimensionality in KNN is a problem because as the number of dimensions in the feature space increases, the distance between data points becomes increasingly large. This means that the KNN algorithm may struggle to find meaningful nearest neighbors in high-dimensional space.\n",
    "\n",
    "    Another issue is that as the number of dimensions increases, the size of the dataset required to achieve a given level of statistical significance also increases. This is because the number of points required to cover the feature space grows exponentially with the number of dimensions. This makes it more difficult to obtain enough data to train a model in high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77d129d-6d9d-46af-8821-4fd0ee782064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ce15a14-3481-4e26-a9e3-a2d2bef64a30",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    Handling missing values is an important preprocessing step in any machine learning algorithm, including KNN. Here are some ways to handle missing values in KNN:\n",
    "\n",
    "    1. Imputation: One approach to handle missing values in KNN is to impute the missing values with some reasonable values based on the available data. This can be done using various techniques, such as mean imputation, median imputation, or mode imputation.\n",
    "\n",
    "    2. Dropping missing values: Another approach is to drop the samples or features that contain missing values. However, this approach may lead to loss of valuable information, especially if the missing values are randomly distributed.\n",
    "\n",
    "    3. KNN-based imputation: This is a more advanced approach that involves using the KNN algorithm itself to impute the missing values. In this approach, the missing values are replaced with the mean or median value of the K-nearest neighbors of the sample with missing values. This approach can be effective when the missing values are not completely random.\n",
    "\n",
    "    Use of advanced imputation techniques: Other advanced techniques can also be used for imputing missing values, such as matrix completion or deep learning-based imputation. These techniques can be more computationally intensive but may provide better imputation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606371f-f12f-4da9-a636-20de4157da5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45ea4cee-673f-4116-90a1-a531c9a44cae",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for \n",
    "which type of problem?\n",
    "\n",
    "Ans: \n",
    "\n",
    "    1. Output: The main difference between the KNN classifier and regressor is the type of output they produce. The KNN classifier produces a categorical output (i.e., a class label), while the KNN regressor produces a continuous output (i.e., a numerical value).\n",
    "\n",
    "    2. Evaluation metrics: The evaluation metrics used to measure the performance of KNN classifier and regressor are different. The most common evaluation metric for KNN classifier is accuracy, while the most common evaluation metrics for KNN regressor are mean squared error (MSE) and mean absolute error (MAE).\n",
    "\n",
    "    3. Data distribution: Another important difference is that the KNN classifier is more suitable for data that has distinct classes and is not continuous, while the KNN regressor is more suitable for data that has a continuous distribution.\n",
    "\n",
    "    4. K value selection: In KNN classifier, the choice of K value is important in order to balance the bias-variance trade-off. For KNN regressor, the optimal K value also depends on the number of samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b23bad-8be1-443b-80d2-87a0546a683f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "760dde3e-6d37-4ef4-b653-f296402ef1b2",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "Ans:\n",
    "    \n",
    "    Strengths of KNN algorithm:\n",
    "\n",
    "        1. Simple and easy to understand: The KNN algorithm is easy to implement and understand, making it a popular choice for beginners in machine learning.\n",
    "\n",
    "        2. Non-parametric: KNN is a non-parametric algorithm, which means that it does not make any assumptions about the underlying data distribution, making it suitable for data that may not follow a particular distribution.\n",
    "\n",
    "        3. Can handle both classification and regression problems: KNN can be used for both classification and regression problems, making it versatile.\n",
    "\n",
    "        4. Doesn't require training: KNN does not require training on the data, making it fast and efficient.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Weaknesses of KNN algorithm:\n",
    "\n",
    "        1. Computationally intensive: The KNN algorithm can be computationally intensive, especially when the dataset is large, as it requires calculating the distance between each sample and its neighbors.\n",
    "\n",
    "        2. Sensitivity to outliers: KNN is sensitive to outliers and noisy data points, which can significantly affect its performance.\n",
    "\n",
    "        3. Curse of dimensionality: KNN is affected by the curse of dimensionality, meaning that its performance can degrade when working with high-dimensional data.\n",
    "\n",
    "        4. Need to choose the value of K: Choosing the optimal value of K can be challenging, as it requires balancing the bias-variance trade-off.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    To address the weaknesses of KNN algorithm, several techniques can be used:\n",
    "\n",
    "        1. Use of distance-weighted voting: Distance-weighted voting can be used to give more weight to samples that are closer to the query point. This can help to mitigate the effects of outliers and improve the performance of the algorithm.\n",
    "\n",
    "        2. Use of feature selection or dimensionality reduction: Dimensionality reduction or feature selection techniques can be used to reduce the dimensionality of the data and mitigate the curse of dimensionality.\n",
    "\n",
    "        3. Use of ensemble techniques: Ensemble techniques such as bagging or boosting can be used to combine multiple KNN models and improve their performance.\n",
    "\n",
    "        4. Use of advanced distance metrics: Advanced distance metrics such as Mahalanobis distance can be used to account for the covariance structure of the data and improve the performance of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d52ffa-9a31-4c03-8e01-b1aacc7ae798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "722022c5-a83e-4774-945d-3de855547765",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "Ans:\n",
    "\n",
    "    Euclidean distance:\n",
    "\n",
    "        Euclidean distance is the straight-line distance between two points in Euclidean space. It is calculated as the square root of the sum of the squared differences between each coordinate of the two points. In other words, if we have two points (x1, y1) and (x2, y2), the Euclidean distance between them is:\n",
    "\n",
    "        sqrt((x1-x2)^2 + (y1-y2)^2)\n",
    "\n",
    "\n",
    "    Manhattan distance:\n",
    "\n",
    "        Manhattan distance measures the distance between two points by summing the absolute differences between their coordinates. In other words, if we have two points (x1, y1) and (x2, y2), the Manhattan distance between them is:\n",
    "\n",
    "        abs(x1-x2) + abs(y1-y2)\n",
    "\n",
    "    The main difference between Euclidean and Manhattan distance is that Euclidean distance is more sensitive to the overall difference in magnitudes between the two points, while Manhattan distance is more sensitive to the difference in coordinates along each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958211af-0ec1-43d8-aaab-a63fd2142e99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3f2e28d-a1a2-41bb-b6af-5b198bcc9b8d",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "Ans: \n",
    "    \n",
    "\n",
    "    Feature scaling is an important preprocessing step in KNN algorithm that involves transforming the features to a common scale. The reason for this is that the KNN algorithm is based on distance measures, and if the features are not on a common scale, the features with larger ranges can dominate the distance calculations and lead to biased results.\n",
    "\n",
    "    For example, consider a dataset with two features, age and income, where age ranges from 18 to 70 and income ranges from 20,000 to 100,000. If we use the KNN algorithm without feature scaling, the income feature will dominate the distance calculations, even though age may be equally important for the classification task.\n",
    "\n",
    "    There are several methods for feature scaling in KNN algorithm. One common method is to normalize the features to have a mean of zero and a standard deviation of one, a technique known as standardization or Z-score normalization. Another method is to rescale the features to a specific range, typically between zero and one, a technique known as min-max normalization.\n",
    "\n",
    "    By scaling the features, we can ensure that each feature contributes equally to the distance calculations and avoid bias in the results. Therefore, feature scaling is an essential step in the preprocessing of data for the KNN algorithm, and it can significantly improve the accuracy of the model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38023a31-666c-48ea-b622-156b369a8875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
